# Strix Environment Configuration Example
# Copy this file to .env and fill in your values

# ================================
# REQUIRED: LLM Configuration
# ================================

# Model name to use with litellm (e.g., 'openai/gpt-4', 'anthropic/claude-3-opus')
STRIX_LLM=openai/gpt-4

# ================================
# OPTIONAL: LLM API Configuration
# ================================

# API key for the LLM provider (not needed for local models, Vertex AI, AWS, etc.)
LLM_API_KEY=your-api-key-here

# Custom API base URL if using local models (e.g., Ollama, LMStudio)
# Examples:
#   - Ollama: http://localhost:11434
#   - LMStudio: http://localhost:1234/v1
#   - OpenAI: https://api.openai.com/v1
LLM_API_BASE=

# Alternative API base configurations (if needed)
OPENAI_API_BASE=
LITELLM_BASE_URL=
OLLAMA_API_BASE=

# ================================
# OPTIONAL: Web Search with Perplexity
# ================================

# API key for Perplexity AI web search (enables real-time research)
PERPLEXITY_API_KEY=

# ================================
# OPTIONAL: Browserless Integration
# ================================

# WebSocket URL for browserless instance
# If not set, Strix will launch a local browser
# 
# Examples:
#   - Local browserless: ws://localhost:3000
#   - Docker compose: ws://browserless:3000
#   - Remote instance: ws://browserless.example.com:3000
#   - With auth token: ws://localhost:3000?token=your-token-here
STRIX_BROWSERLESS_BASE=

# Browser type for browserless: 'chromium' or 'firefox'
# Default: chromium
STRIX_BROWSERLESS_TYPE=chromium

# Authentication token for browserless instance (if required)
# Leave empty for instances without authentication
STRIX_BROWSERLESS_TOKEN=

# ================================
# Strix Runtime Configuration
# ================================

# Sandbox mode (set to true when running in container)
STRIX_SANDBOX_MODE=false

# Python path
PYTHONPATH=/app

# ================================
# Quick Start Examples
# ================================

# Example 1: Local setup with OpenAI
# STRIX_LLM=openai/gpt-4
# LLM_API_KEY=sk-...

# Example 2: Local setup with Ollama (no API key needed)
# STRIX_LLM=ollama/llama3
# OLLAMA_API_BASE=http://localhost:11434

# Example 3: Using browserless locally
# STRIX_LLM=openai/gpt-4
# LLM_API_KEY=sk-...
# STRIX_BROWSERLESS_BASE=ws://localhost:3000
# STRIX_BROWSERLESS_TYPE=chromium
# STRIX_BROWSERLESS_TOKEN=  # Leave empty if no auth required

# Example 4: Using browserless with Firefox
# STRIX_LLM=openai/gpt-4
# LLM_API_KEY=sk-...
# STRIX_BROWSERLESS_BASE=ws://localhost:3000
# STRIX_BROWSERLESS_TYPE=firefox
# STRIX_BROWSERLESS_TOKEN=

# Example 5: Docker Compose setup
# STRIX_LLM=openai/gpt-4
# LLM_API_KEY=sk-...
# STRIX_BROWSERLESS_BASE=ws://browserless:3000
# STRIX_BROWSERLESS_TYPE=chromium
# STRIX_BROWSERLESS_TOKEN=
# STRIX_SANDBOX_MODE=true

# Example 6: Using authenticated remote browserless
# STRIX_LLM=openai/gpt-4
# LLM_API_KEY=sk-...
# STRIX_BROWSERLESS_BASE=wss://browserless.example.com
# STRIX_BROWSERLESS_TYPE=chromium
# STRIX_BROWSERLESS_TOKEN=your-browserless-token-here
